{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxqc8Nqk4OQ6"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2h92tzp4ORA"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANc78m4I4ORB",
        "outputId": "b29da2c2-2db4-4f67-f97d-1a05139c4765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-17 09:28:24--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M   204MB/s    in 0.2s    \n",
            "\n",
            "2025-09-17 09:28:24 (204 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "  inflating: imdb-reviews-pt-br.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFwC-jeX4ORD"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6Vj-svt4ORE",
        "outputId": "f56fcfa5-5f80-452c-97f2-11a95bcdc66a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXYqtXc4ORF"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u8R_kQ4z4ORG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojQyPiPy4ORH"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método read.csv do spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aXw_5G4t4ORH"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXNWwXPa4ORJ"
      },
      "source": [
        "# Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FciHOuL_4ORJ"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fCDu892k4ORK"
      },
      "outputs": [],
      "source": [
        "def map1(x):\n",
        "  if x['sentiment'] == 'neg':\n",
        "    return ('neg', int(x['id']))\n",
        "  else:\n",
        "    return ('other', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_LtP4em4ORL"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2PWcR6kb4ORN"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x,y):\n",
        "  return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi1fRMK4ORO"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF9IAT-h4ORO",
        "outputId": "dcc99f6f-4383-483a-f2a7-9624cc3eace3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------+---+\n",
            "|Soma dos IDs Negativos| RU|\n",
            "+----------------------+---+\n",
            "|             459568555|  0|\n",
            "+----------------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ru = 0000000\n",
        "rdd = imdb_df.rdd\n",
        "\n",
        "# Aplicando a função de map\n",
        "mapped_rdd = rdd.map(map1)\n",
        "\n",
        "# Filtrando apenas os sentimentos negativos\n",
        "negative_rdd = mapped_rdd.filter(lambda x: x[0] == 'neg')\n",
        "\n",
        "# Aplicando a função de reduce\n",
        "result = negative_rdd.map(lambda x: x[1]).reduce(reduceByKey1)\n",
        "\n",
        "# Visualizando o resultado\n",
        "result_df = spark.createDataFrame([(result,ru)], [\"Soma dos IDs Negativos\", \"RU\"])\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOWKcSX8TTU_",
        "outputId": "7f2f50b4-7cf3-494e-e19d-ddaba5c3c8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A soma dos IDs dos filmes classificados como negativos é: 459568555\n"
          ]
        }
      ],
      "source": [
        "##VERIFICACAO DO RESULTADO EM PYTHON\n",
        "import pandas as pd\n",
        "\n",
        "def soma_ids_negativos(csv_file):\n",
        "    # Carregar o arquivo CSV\n",
        "    df = pd.read_csv('imdb-reviews-pt-br.csv')\n",
        "\n",
        "    # Filtrar os filmes com avaliação negativa\n",
        "    negativos = df[df['sentiment'] == 'neg']\n",
        "\n",
        "    # Somar os valores da coluna 'id'\n",
        "    soma_ids = negativos['id'].sum()\n",
        "\n",
        "    return soma_ids\n",
        "\n",
        "# Exemplo de uso\n",
        "csv_file = 'imdb-reviews-pt-br.csv'\n",
        "resultado = soma_ids_negativos(csv_file)\n",
        "print(f\"A soma dos IDs dos filmes classificados como negativos é: {resultado}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WOTvPSf4ORO"
      },
      "source": [
        "# Questão 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo7tsdAZ4ORO"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e\n",
        "uma tupla com a soma das palavras de cada texto como valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XcmRlNHn4ORP"
      },
      "outputs": [],
      "source": [
        "def map2(row):\n",
        "  if row['sentiment'] == 'neg':\n",
        "    words_count_pt = len(re.findall(r'\\w+', row['text_pt']))\n",
        "    words_count_en = len(re.findall(r'\\w+', row['text_en']))\n",
        "    return ('neg', (words_count_en, words_count_pt))\n",
        "  else:\n",
        "    return ('other', (0,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8I0vt5a4ORP"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eLKYUxVR4ORP"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x,y):\n",
        "  # x e y são tuplas no formato ('sentiment', (words_count_pt, words_count_en))\n",
        "  #sentiment = x[0]\n",
        "  total_words_pt = x[0] + y[0]\n",
        "  total_words_en = x[1] + y[1]\n",
        "  return (total_words_pt, total_words_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9hclLCx4ORQ"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSQHxx6G4ORQ",
        "outputId": "5ff0fb68-367d-4718-d0b3-cbbceae34952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------+---+\n",
            "|Diferença de palavras (português - inglês):| RU|\n",
            "+-------------------------------------------+---+\n",
            "|                                      34971|  0|\n",
            "+-------------------------------------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Coloque aqui suas linhas de código final\n",
        "# \"id\",\"text_en\",\"text_pt\",\"sentiment\" colunas do csv\n",
        "\n",
        "import re # Import the regular expression module\n",
        "rdd2 = imdb_df.rdd\n",
        "ru = 0000000\n",
        "\n",
        "# Aplicando a função de map\n",
        "mapped_rdd2 = rdd2.map(lambda row: map2(row.asDict()))\n",
        "\n",
        "# Filtrando apenas os sentimentos negativos\n",
        "negative_rdd2 = mapped_rdd2.filter(lambda x: x[0] == 'neg')\n",
        "\n",
        "# Aplicando a função de reduce\n",
        "reduced_rdd2 = negative_rdd2.map(lambda x: x[1]).reduce(reduceByKey2)\n",
        "\n",
        "# Coletando e visualizando o resultado\n",
        "total_words_pt, total_words_en = reduced_rdd2\n",
        "\n",
        "# Calculando a diferença entre palavras em português e inglês\n",
        "difference = total_words_pt - total_words_en\n",
        "\n",
        "# Visualizando o resultado\n",
        "result_df = spark.createDataFrame([(difference,ru)], [\"Diferença de palavras (português - inglês):\", \"RU\"])\n",
        "result_df.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
