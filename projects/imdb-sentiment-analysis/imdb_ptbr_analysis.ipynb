{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxqc8Nqk4OQ6"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2h92tzp4ORA"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANc78m4I4ORB",
        "outputId": "d9c51696-98c8-4b44-bde7-4b52945e0924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-08-29 10:37:22--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M   168MB/s    in 0.3s    \n",
            "\n",
            "2024-08-29 10:37:23 (168 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "replace imdb-reviews-pt-br.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFwC-jeX4ORD"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6Vj-svt4ORE",
        "outputId": "fb6b4051-2f53-40c0-f059-6e264c5e9691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=a2c243f6f8b747440d8ca8c90f359f01a453bcb532864efdb4dfae1b2c59652a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXYqtXc4ORF"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8R_kQ4z4ORG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojQyPiPy4ORH"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método read.csv do spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXw_5G4t4ORH"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXNWwXPa4ORJ"
      },
      "source": [
        "# Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FciHOuL_4ORJ"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCDu892k4ORK"
      },
      "outputs": [],
      "source": [
        "def map1(x):\n",
        "  if x['sentiment'] == 'neg':\n",
        "    return ('neg', int(x['id']))\n",
        "  else:\n",
        "    return ('other', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_LtP4em4ORL"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PWcR6kb4ORN"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x,y):\n",
        "  return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi1fRMK4ORO"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF9IAT-h4ORO",
        "outputId": "1c52c6b4-fd37-48d0-82fb-285eadfc0ff6"
      },
      "outputs": [],
      "source": [
        "ru = 0000000\n",
        "rdd = imdb_df.rdd\n",
        "\n",
        "# Aplicando a função de map\n",
        "mapped_rdd = rdd.map(map1)\n",
        "\n",
        "# Filtrando apenas os sentimentos negativos\n",
        "negative_rdd = mapped_rdd.filter(lambda x: x[0] == 'neg')\n",
        "\n",
        "# Aplicando a função de reduce\n",
        "result = negative_rdd.map(lambda x: x[1]).reduce(reduceByKey1)\n",
        "\n",
        "# Visualizando o resultado\n",
        "result_df = spark.createDataFrame([(result,ru)], [\"Soma dos IDs Negativos\", \"RU\"])\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOWKcSX8TTU_",
        "outputId": "e7e6beb7-1a52-4f20-99c5-40caea3b26b7"
      },
      "outputs": [],
      "source": [
        "##VERIFICACAO DO RESULTADO EM PYTHON\n",
        "#import pandas as pd\n",
        "\n",
        "def soma_ids_negativos(csv_file):\n",
        "    # Carregar o arquivo CSV\n",
        "    df = pd.read_csv('imdb-reviews-pt-br.csv')\n",
        "\n",
        "    # Filtrar os filmes com avaliação negativa\n",
        "    negativos = df[df['sentiment'] == 'neg']\n",
        "\n",
        "    # Somar os valores da coluna 'id'\n",
        "    soma_ids = negativos['id'].sum()\n",
        "\n",
        "    return soma_ids\n",
        "\n",
        "# Exemplo de uso\n",
        "csv_file = 'imdb-reviews-pt-br.csv'\n",
        "resultado = soma_ids_negativos(csv_file)\n",
        "print(f\"A soma dos IDs dos filmes classificados como negativos é: {resultado}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WOTvPSf4ORO"
      },
      "source": [
        "# Questão 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo7tsdAZ4ORO"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e\n",
        "uma tupla com a soma das palavras de cada texto como valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcmRlNHn4ORP"
      },
      "outputs": [],
      "source": [
        "def map2(row):\n",
        "  if row['sentiment'] == 'neg':\n",
        "    words_count_pt = len(re.findall(r'\\w+', row['text_pt']))\n",
        "    words_count_en = len(re.findall(r'\\w+', row['text_en']))\n",
        "    return ('neg', (words_count_en, words_count_pt))\n",
        "  else:\n",
        "    return ('other', (0,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8I0vt5a4ORP"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLKYUxVR4ORP"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x,y):\n",
        "  # x e y são tuplas no formato ('sentiment', (words_count_pt, words_count_en))\n",
        "  #sentiment = x[0]\n",
        "  total_words_pt = x[0] + y[0]\n",
        "  total_words_en = x[1] + y[1]\n",
        "  return (total_words_pt, total_words_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9hclLCx4ORQ"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSQHxx6G4ORQ",
        "outputId": "2378af92-6312-4524-dd3a-3d4b4482ffaf"
      },
      "outputs": [],
      "source": [
        "# Coloque aqui suas linhas de código final\n",
        "# \"id\",\"text_en\",\"text_pt\",\"sentiment\" colunas do csv\n",
        "\n",
        "import re # Import the regular expression module\n",
        "rdd2 = imdb_df.rdd\n",
        "ru = 0000000\n",
        "\n",
        "# Aplicando a função de map\n",
        "mapped_rdd2 = rdd2.map(lambda row: map2(row.asDict()))\n",
        "\n",
        "# Filtrando apenas os sentimentos negativos\n",
        "negative_rdd2 = mapped_rdd2.filter(lambda x: x[0] == 'neg')\n",
        "\n",
        "# Aplicando a função de reduce\n",
        "reduced_rdd2 = negative_rdd2.map(lambda x: x[1]).reduce(reduceByKey2)\n",
        "\n",
        "# Coletando e visualizando o resultado\n",
        "total_words_pt, total_words_en = reduced_rdd2\n",
        "\n",
        "# Calculando a diferença entre palavras em português e inglês\n",
        "difference = total_words_pt - total_words_en\n",
        "\n",
        "# Visualizando o resultado\n",
        "result_df = spark.createDataFrame([(difference,ru)], [\"Diferença de palavras (português - inglês):\", \"RU\"])\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuqh9N9EhBPU",
        "outputId": "430bf4be-8f0f-4fb5-8ed9-014e1373137f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-45a13645567f>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  negative_reviews['word_count_pt'] = negative_reviews['text_pt'].apply(count_words)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total de palavras nos textos negativos em português: 5458578\n",
            "Total de palavras nos textos negativos em inglês: 5493543\n",
            "Diferença de palavras (português - inglês): -34965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-45a13645567f>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  negative_reviews['word_count_en'] = negative_reviews['text_en'].apply(count_words)\n"
          ]
        }
      ],
      "source": [
        "##VERIFICACAO EM PYTHON\n",
        "#import pandas as pd\n",
        "#import re\n",
        "\n",
        "def count_words(text):\n",
        "    words = re.findall(r'\\w+', text)\n",
        "    return len(words)\n",
        "\n",
        "def main():\n",
        "    # Carregar o arquivo CSV\n",
        "    df = pd.read_csv('imdb-reviews-pt-br.csv', encoding='UTF-8', quotechar='\"', escapechar='\\\\')\n",
        "\n",
        "    # Filtrar os textos negativos\n",
        "    negative_reviews = df[df['sentiment'] == 'neg']\n",
        "\n",
        "    # Contar palavras nos textos negativos em português\n",
        "    negative_reviews['word_count_pt'] = negative_reviews['text_pt'].apply(count_words)\n",
        "\n",
        "    # Contar palavras nos textos negativos em inglês\n",
        "    negative_reviews['word_count_en'] = negative_reviews['text_en'].apply(count_words)\n",
        "\n",
        "    # Somar o total de palavras em português e inglês\n",
        "    total_words_pt = negative_reviews['word_count_pt'].sum()\n",
        "    total_words_en = negative_reviews['word_count_en'].sum()\n",
        "\n",
        "    # Calcular a diferença\n",
        "    difference = total_words_pt - total_words_en\n",
        "\n",
        "    # Exibir os resultados\n",
        "    print(f\"Total de palavras nos textos negativos em português: {total_words_pt}\")\n",
        "    print(f\"Total de palavras nos textos negativos em inglês: {total_words_en}\")\n",
        "    print(f\"Diferença de palavras (português - inglês): {difference}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
